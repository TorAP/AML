{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpeU7dnUDaB3kHBqJsMcko",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TorAP/AML/blob/main/simple_Gan_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OEVtwRgal6s2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Dense, \n",
        "                                     BatchNormalization, \n",
        "                                     LeakyReLU, \n",
        "                                     Reshape, \n",
        "                                     Conv2DTranspose,\n",
        "                                     Conv2D,\n",
        "                                     Dropout,\n",
        "                                     Flatten)\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# underscore to omit the label arrays\n",
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data() \n",
        "\n",
        "\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
        "\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwDlpwIYriak",
        "outputId": "99ae6530-a781-4181-dd37-31a097a6218f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "qZy-eIjbnDJL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_generator_model()"
      ],
      "metadata": {
        "id": "vth97bVDnGAd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random noise and generate a sample\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "# Visualize the generated sample\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "PjqAblhlnKCh",
        "outputId": "63d1d60e-d69b-4c18-d735-57dc5dd41007"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f94806ae790>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "pJHj77p9nbBU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = make_discriminator_model()"
      ],
      "metadata": {
        "id": "vhDZ28sIngNO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC0HYeV2n1G-",
        "outputId": "0b0406a9-4699-4378-8e08-bc6b8b7dff59"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[-0.00027666]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "JHtwA3DapZVv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "metadata": {
        "id": "3W8Y_0iIp1eN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 60\n",
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "num_examples_to_generate = 16\n",
        "noise_dim = 100\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "metadata": {
        "id": "25bYnTD6p8gs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.function annotation causes the function \n",
        "# to be \"compiled\" as part of the training\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "  \n",
        "    # 1 - Create a random noise to feed it into the model\n",
        "    # for the image generation\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    \n",
        "    # 2 - Generate images and calculate loss values\n",
        "    # GradientTape method records operations for automatic differentiation.\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    # 3 - Calculate gradients using loss values and model variables\n",
        "    # \"gradient\" method computes the gradient using \n",
        "    # operations recorded in context of this tape (gen_tape and disc_tape).\n",
        "    \n",
        "    # It accepts a target (e.g., gen_loss) variable and \n",
        "    # a source variable (e.g.,generator.trainable_variables)\n",
        "    # target --> a list or nested structure of Tensors or Variables to be differentiated.\n",
        "    # source --> a list or nested structure of Tensors or Variables.\n",
        "    # target will be differentiated against elements in sources.\n",
        "\n",
        "    # \"gradient\" method returns a list or nested structure of Tensors  \n",
        "    # (or IndexedSlices, or None), one for each element in sources. \n",
        "    # Returned structure is the same as the structure of sources.\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, \n",
        "                                               generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
        "                                                discriminator.trainable_variables)\n",
        "    \n",
        "    # 4 - Process  Gradients and Run the Optimizer\n",
        "    # \"apply_gradients\" method processes aggregated gradients. \n",
        "    # ex: optimizer.apply_gradients(zip(grads, vars))\n",
        "    \"\"\"\n",
        "    Example use of apply_gradients:\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n",
        "    # Processing aggregated gradients.\n",
        "    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\n",
        "    \"\"\"\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
      ],
      "metadata": {
        "id": "Zv6ZIqfAqdBQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from IPython import display # A command shell for interactive computing in Python.\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  # A. For each epoch, do the following:\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    # 1 - For each batch of the epoch, \n",
        "    for image_batch in dataset:\n",
        "      # 1.a - run the custom \"train_step\" function\n",
        "      # we just declared above\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # 2 - Produce images for the GIF as we go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # 3 - Save the model every 5 epochs as \n",
        "    # a checkpoint, which we will use later\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    # 4 - Print out the completed epoch no. and the time spent\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # B. Generate a final image after the training is completed\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)"
      ],
      "metadata": {
        "id": "as4OxfC-qli9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  # 1 - Generate images\n",
        "  predictions = model(test_input, training=False)\n",
        "  # 2 - Plot the generated images\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "  # 3 - Save the generated images\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "tpyAs4csqxM1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "yQ6euiaGq9R1",
        "outputId": "32b5a860-60f1-4b95-85a6-a0d498a6c379"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d152560ca122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-704948122ae5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;31m# 1.a - run the custom \"train_step\" function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0;31m# we just declared above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 2 - Produce images for the GIF as we go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5vFH7S7rL_Z",
        "outputId": "14f312b9-4e3f-4d07-a1f1-6469bdcccc40"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f9481077450>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PIL is a library which may open different image file formats\n",
        "import PIL \n",
        "# Display a single image using the epoch number\n",
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n",
        "display_image(EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "0BLpxga5rOWn",
        "outputId": "2d9c488d-00b7-4fbf-a11a-bad63f300add"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7793c09281f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_at_epoch_{:04d}.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-7793c09281f5>\u001b[0m in \u001b[0;36mdisplay_image\u001b[0;34m(epoch_no)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Display a single image using the epoch number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_at_epoch_{:04d}.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'image_at_epoch_0060.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob # The glob module is used for Unix style pathname pattern expansion.\n",
        "import imageio # The library that provides an easy interface to read and write a wide range of image data\n",
        "\n",
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  # image = imageio.imread(filename)\n",
        "  # writer.append_data(image)\n",
        "  \n",
        "display.Image(open('dcgan.gif','rb').read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "PFjZK3oGrRiS",
        "outputId": "9a2a805c-31e6-4a77-c080-0f9ef8853b09"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "Ow==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}