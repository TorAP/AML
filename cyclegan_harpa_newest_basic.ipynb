{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TorAP/AML/blob/main/cyclegan_harpa_newest_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "83VP5Gfjx5-S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\46723\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import random, torch, os, numpy as np\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCuIy-4-GHl6",
    "outputId": "77d34c2a-e774-40c8-c1f7-5f8f4dacb995"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpimg\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#! gdown 1PaI7yOFltWp9UX_NUiznQYaRa0pYUItH\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#!unzip data.zip\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      7\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munzip drive/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy Drive\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "#! gdown 1PaI7yOFltWp9UX_NUiznQYaRa0pYUItH\n",
    "#!unzip data.zip\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!unzip drive/\"My Drive\"/data.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oRhIAkmKAZXH"
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TRAIN_DIR = \"/content/data/train\"\n",
    "VAL_DIR = \"/content/data/val\"\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "LAMBDA_IDENTITY = 0.03\n",
    "LAMBDA_CYCLE = 10\n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 1\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_GEN_P = \"genp.pth.tar\"\n",
    "CHECKPOINT_GEN_M = \"genm.pth.tar\"\n",
    "CHECKPOINT_DISC_P = \"discp.pth.tar\"\n",
    "CHECKPOINT_DISC_M = \"discm.pth.tar\"\n",
    "\n",
    "transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(width=256, height=256),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n",
    "        ToTensorV2(),\n",
    "     ],\n",
    "    additional_targets={\"image0\": \"image\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "_v62Y43Y_zJQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "#instance norm\n",
    "\n",
    "#They mentioned in the paper that using padding_mode=\"reflect\" helped to reduce artifacts\n",
    "#The 1 is the padding\n",
    "# Padding reflect means that the padded pixel values are reflected around the image. e.g. for padding = 1, the padded pixel on the right \n",
    "#would be pixel n-1, and on the left, would be pixel in position 1 (starting count at 0). \n",
    "class Block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride):\n",
    "    super().__init__()\n",
    "    kernel_size = 4\n",
    "    self.conv = nn.Sequential( # bias = True such that a learnable bias is added to the output.\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=1, bias=True, padding_mode=\"reflect\"),\n",
    "        nn.InstanceNorm2d(out_channels), # Instance normalisation\n",
    "        nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    return self.conv(x);\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "    super().__init__()\n",
    "    self.initial = nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels = in_channels,\n",
    "            out_channels = features[0],\n",
    "            kernel_size = 4, \n",
    "            stride = 2,\n",
    "            padding = 1,\n",
    "            padding_mode=\"reflect\",\n",
    "        ),\n",
    "        nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "    )\n",
    "\n",
    "    layers = []\n",
    "    in_channels = features[0]\n",
    "    # loop over the features [64, 128, 256, 512] and create conv layers with \n",
    "    # the loop creates 4 conv layers, and then appends them to the 'layers' list.\n",
    "    for feature in features[1:]:\n",
    "      layers.append(Block(in_channels, feature, stride=1 if feature==features[-1] else 2)) # stride 1 if feature = 512, else 2.\n",
    "    # above layer called 3 times, with in_layers 64, 128, 256, respectively\n",
    "      in_channels = feature\n",
    "    layers.append(nn.Conv2d(in_channels, out_channels=1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\")) # in channels = 512\n",
    "    self.model = nn.Sequential(*layers) # self.model inherents from nn.Module\n",
    "    # last layer in the above does not include InstanceNorm or LeakyReLU because we call nn.Conv2d instead of Block().\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.initial(x)\n",
    "    return torch.sigmoid(self.model(x)) # calling the sigmoid function on the model returns a tensor with all numbers compressed between [0,1]\n",
    "\n",
    "def test():\n",
    "  x = torch.randn((5, 3, 256, 256)) # creating random noise with the size (5, 3, 256, 256). Is 5 the batch size?\n",
    "# we convert 5 images of 3 channels and size 256x256 to 5 images of 1 channel and size 30x30.\n",
    "  model = Discriminator(in_channels=3)\n",
    "  preds = model(x) # this is the same as preds = Discriminator(in_channels=3)(x)\n",
    "  print(preds.shape)\n",
    "\n",
    "test() # the test output tensor has 1 channel because this is hardcoded into the last layer of the discriminator's model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "soYHn1KZLAq0"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, **kwargs), # what does ConvTranspose2d do?\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) if use_act else nn.Identity() # the Identity() essentially does nothing. it returns its input.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResidualBlock(nn.Module): # keeps in_channels the same as out_channels.\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, padding=1),\n",
    "            ConvBlock(channels, channels, use_act=False, kernel_size=3, padding=1), # this layer has no ReLU, just an Identity.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels, num_features = 64, num_residuals=9):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n",
    "            nn.InstanceNorm2d(num_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.down_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(num_features, num_features*2, kernel_size=3, stride=2, padding=1),\n",
    "                ConvBlock(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1),\n",
    "            ]\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_features*4) for _ in range(num_residuals)]\n",
    "        )\n",
    "        self.up_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(num_features*4, num_features*2, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                ConvBlock(num_features*2, num_features*1, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.last = nn.Conv2d(num_features*1, img_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for layer in self.down_blocks:\n",
    "            x = layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        for layer in self.up_blocks:\n",
    "            x = layer(x)\n",
    "        return torch.tanh(self.last(x))\n",
    "\n",
    "def test_gen():\n",
    "    img_channels = 3\n",
    "    img_size = 256\n",
    "    x = torch.randn((2, img_channels, img_size, img_size))\n",
    "    gen = Generator(img_channels, 9)\n",
    "    print(gen(x).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iFbu9MxJLHNq"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "class PictureMonetDataset(Dataset):\n",
    "    def __init__(self, root_monet, root_picture, transform=None):\n",
    "        self.root_monet = root_monet\n",
    "        self.root_picture = root_picture\n",
    "        self.transform = transform\n",
    "\n",
    "        self.monet_images = os.listdir(root_monet)\n",
    "        self.picture_images = os.listdir(root_picture)\n",
    "        self.length_dataset = max(len(self.monet_images), len(self.picture_images)) # 1000, 1500\n",
    "        self.monet_len = len(self.monet_images)\n",
    "        self.picture_len = len(self.picture_images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        monet_img = self.monet_images[index % self.monet_len]\n",
    "        picture_img = self.picture_images[index % self.picture_len]\n",
    "\n",
    "        monet_path = os.path.join(self.root_monet, monet_img)\n",
    "        picture_path = os.path.join(self.root_picture, picture_img)\n",
    "\n",
    "        monet_img = np.array(Image.open(monet_path).convert(\"RGB\"))\n",
    "        picture_img = np.array(Image.open(picture_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=monet_img, image0=picture_img)\n",
    "            monet_img = augmentations[\"image\"]\n",
    "            picture_img = augmentations[\"image0\"]\n",
    "\n",
    "        return monet_img, picture_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wdooi3F5LL8B"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UTILS\n",
    "\"\"\"\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "v96jruq6K26-"
   },
   "outputs": [],
   "source": [
    "# Show samples\n",
    "\n",
    "def show_samples(dataset):\n",
    "  f, axarr = plt.subplots(2,2)\n",
    "  axarr[0,0].imshow(dataset.__getitem__(0)[0])\n",
    "  axarr[0,1].imshow(dataset.__getitem__(1)[0])\n",
    "  axarr[1,0].imshow(dataset.__getitem__(0)[1])\n",
    "  axarr[1,1].imshow(dataset.__getitem__(1)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "st-BEntFLPj7"
   },
   "outputs": [],
   "source": [
    "\"\"\" TRAINING \"\"\"\n",
    "\n",
    "\n",
    "def train_fn(disc_P, disc_M, gen_M, gen_P, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler):\n",
    "    P_reals = 0\n",
    "    P_fakes = 0\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for idx, (monet, picture) in enumerate(loop):\n",
    "        monet = monet.to(DEVICE)\n",
    "        picture = picture.to(DEVICE)\n",
    "\n",
    "        # Train Discriminators P and M\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake_picture = gen_P(monet)\n",
    "            D_P_real = disc_P(picture) # D_P_real is a 30x30x1 tensor (channel = 1, size = 30x30). same applies for D_P_fake\n",
    "            D_P_fake = disc_P(fake_picture.detach())\n",
    "            P_reals += D_P_real.mean().item() # global average pooling? take the average of the 30x30 image and append value to a variable.\n",
    "            P_fakes += D_P_fake.mean().item() # global average pooling?\n",
    "            D_P_real_loss = mse(D_P_real, torch.ones_like(D_P_real)) # returns a torch with 1s with the same size as D_P_real\n",
    "            D_P_fake_loss = mse(D_P_fake, torch.zeros_like(D_P_fake)) # returns a torch with 0s with same size as D_P_fake\n",
    "            D_P_loss = D_P_real_loss + D_P_fake_loss # sums the losses of identifying fake image and identifying real image, respectively.\n",
    "\n",
    "            fake_monet = gen_M(picture)\n",
    "            D_M_real = disc_M(monet)\n",
    "            D_M_fake = disc_M(fake_monet.detach())\n",
    "            D_M_real_loss = mse(D_M_real, torch.ones_like(D_M_real))\n",
    "            D_M_fake_loss = mse(D_M_fake, torch.zeros_like(D_M_fake))\n",
    "            D_M_loss = D_M_real_loss + D_M_fake_loss\n",
    "\n",
    "            # put it togethor\n",
    "            D_loss = (D_P_loss + D_M_loss)/2\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        d_scaler.scale(D_loss).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        # Train Generators P and M\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # adversarial loss for both generators\n",
    "            D_P_fake = disc_P(fake_picture)\n",
    "            D_M_fake = disc_M(fake_monet)\n",
    "            loss_G_P = mse(D_P_fake, torch.ones_like(D_P_fake))\n",
    "            loss_G_M = mse(D_M_fake, torch.ones_like(D_M_fake))\n",
    "\n",
    "            # cycle loss\n",
    "            cycle_monet = gen_M(fake_picture)\n",
    "            cycle_picture = gen_P(fake_monet)\n",
    "            cycle_monet_loss = l1(monet, cycle_monet)\n",
    "            cycle_picture_loss = l1(picture, cycle_picture)\n",
    "\n",
    "            # identity loss (remove these for efficiency if you set lambda_identity=0)\n",
    "            identity_monet = gen_M(monet)\n",
    "            identity_picture = gen_P(picture)\n",
    "            identity_monet_loss = l1(monet, identity_monet)\n",
    "            identity_picture_loss = l1(picture, identity_picture)\n",
    "\n",
    "            # add all togethor\n",
    "            G_loss = (\n",
    "                loss_G_M\n",
    "                + loss_G_P\n",
    "                + cycle_monet_loss * LAMBDA_CYCLE\n",
    "                + cycle_picture_loss * LAMBDA_CYCLE\n",
    "                + identity_picture_loss * LAMBDA_IDENTITY\n",
    "                + identity_monet_loss * LAMBDA_IDENTITY\n",
    "            )\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(G_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        if idx % 200 == 0:\n",
    "            save_image(fake_picture*0.5+0.5, f\"/content/saved_images/photo_{idx}.png\")\n",
    "            save_image(fake_monet*0.5+0.5, f\"/content/saved_images/monet_{idx}.png\")\n",
    "\n",
    "        loop.set_postfix(P_real=P_reals/(idx+1), P_fake=P_fakes/(idx+1))\n",
    "    return G_loss, D_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "hZWoLs0LoeqG"
   },
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, model, optimizer, criterion\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, 'outputs/best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Umwa-c1zooUX"
   },
   "outputs": [],
   "source": [
    "def save_model(epochs, model, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to save the trained model to disk.\n",
    "    \"\"\"\n",
    "    print(f\"Saving final model...\")\n",
    "    torch.save({\n",
    "                'epoch': epochs,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, 'outputs/final_model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "V26i_EiNoqgY"
   },
   "outputs": [],
   "source": [
    "def save_plots(G_loss, D_loss):\n",
    "    \"\"\"\n",
    "    Function to save the loss and accuracy plots to disk.\n",
    "    \"\"\"\n",
    "    \n",
    "    # loss plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        G_loss, color='orange', linestyle='-', \n",
    "        label='Generator Loss'\n",
    "    )\n",
    "    plt.plot(\n",
    "        D_loss, color='red', linestyle='-', \n",
    "        label='Discriminator Loss'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('outputs/loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kkr_DFPyLSGz",
    "outputId": "720fbd21-87e6-4647-94b0-3beadc5163ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      " 23%|██▎       | 1602/6988 [06:31<22:21,  4.01it/s, P_fake=0.443, P_real=0.554]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    disc_P = Discriminator(in_channels=3).to(DEVICE)\n",
    "    disc_M = Discriminator(in_channels=3).to(DEVICE)\n",
    "    gen_M = Generator(img_channels=3, num_residuals=9).to(DEVICE)\n",
    "    gen_P = Generator(img_channels=3, num_residuals=9).to(DEVICE)\n",
    "    opt_disc = optim.Adam( # uses the adam algorithm as optimizer. (an extension of stochastic gradient descent)\n",
    "        list(disc_P.parameters()) + list(disc_M.parameters()), # list with parameters of the discriminator model + list for other discriminator.\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.5, 0.999), # parameters are probably just the weights.\n",
    "    )\n",
    "\n",
    "    opt_gen = optim.Adam(\n",
    "        list(gen_M.parameters()) + list(gen_P.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.5, 0.999),\n",
    "    )\n",
    "    save_best_model = SaveBestModel()\n",
    "    L1 = nn.L1Loss()\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN_P, gen_P, opt_gen, LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN_M, gen_M, opt_gen, LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_DISC_P, disc_P, opt_disc, LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_DISC_M, disc_M, opt_disc, LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "    dataset = PictureMonetDataset(\n",
    "        root_picture=TRAIN_DIR+\"/photo\", root_monet=TRAIN_DIR+\"/monet\", transform=transforms\n",
    "    )\n",
    "    val_dataset = PictureMonetDataset(\n",
    "       root_picture=VAL_DIR+\"/photo\", root_monet=VAL_DIR + \"/monet\", transform=transforms\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    g_scaler = torch.cuda.amp.GradScaler()\n",
    "    d_scaler = torch.cuda.amp.GradScaler()\n",
    "    generator_loss, discriminator_loss = [], []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        G_loss, D_loss = train_fn(disc_P, disc_M, gen_M, gen_P, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler)\n",
    "        generator_loss.append(G_loss)\n",
    "        discriminator_loss.append(D_loss)\n",
    "        if SAVE_MODEL:\n",
    "            save_checkpoint(gen_P, opt_gen, filename=CHECKPOINT_GEN_P)\n",
    "            save_checkpoint(gen_M, opt_gen, filename=CHECKPOINT_GEN_M)\n",
    "            save_checkpoint(disc_P, opt_disc, filename=CHECKPOINT_DISC_P)\n",
    "            save_checkpoint(disc_M, opt_disc, filename=CHECKPOINT_DISC_M)\n",
    "\n",
    "    #if SAVE_MODEL:\n",
    "    #    save_model(NUM_EPOCHS, gen_P, opt_gen)\n",
    "    #    save_model(NUM_EPOCHS, gen_M, opt_gen)\n",
    "    #    save_model(NUM_EPOCHS, disc_P, opt_disc)\n",
    "    #    save_model(NUM_EPOCHS, disc_M, opt_disc)\n",
    "    \n",
    "    print(len(generator_loss))\n",
    "    print(len(discriminator_loss))\n",
    "    save_plots(generator_loss, discriminator_loss)\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMv5V5iagSe2tLPodnfVR9+",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
